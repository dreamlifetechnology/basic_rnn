{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Distinguir si un número es mayor o menor que 0.5\n",
    "\n",
    "* una sola neuronal\n",
    "* sin librerias"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 26,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Época 0 - Pérdida total: 5.1570\n",
      "Época 100 - Pérdida total: 1.4348\n",
      "Época 200 - Pérdida total: 1.0796\n",
      "Época 300 - Pérdida total: 0.9226\n",
      "Época 400 - Pérdida total: 0.8277\n",
      "Época 500 - Pérdida total: 0.7618\n",
      "Época 600 - Pérdida total: 0.7123\n",
      "Época 700 - Pérdida total: 0.6731\n",
      "Época 800 - Pérdida total: 0.6410\n",
      "Época 900 - Pérdida total: 0.6140\n",
      "\n",
      "Pruebas:\n",
      "x = 0.5 → Menor o igual que 1\n",
      "x = 1.0 → Menor o igual que 1\n",
      "x = 1.1 → Mayor que 1\n",
      "x = 1.5 → Mayor que 1\n",
      "x = 2.0 → Mayor que 1\n"
     ]
    }
   ],
   "source": [
    "import random\n",
    "import math\n",
    "\n",
    "# --- Función de activación y su derivada ---\n",
    "# La función sigmoide no es lineal, su pendiente cambia según el valor de z:\n",
    "# Cuando z es muy positivo, la sigmoide está cerca de 1 y su pendiente es casi 0\n",
    "# Cuando z está cerca de 0, la pendiente es máxima (~0.25)\n",
    "# Cuando z es muy negativo, la pendiente es casi 0\n",
    "def sigmoid(x):\n",
    "    return 1 / (1 + math.exp(-x))\n",
    "\n",
    "# sigmoid_derivative(y_predicho) devuelve la pendiente (la derivada) de la función sigmoide en el punto actual de salida.\n",
    "# Al multiplicar esa pendiente por el error (la diferencia entre la salida deseada y la salida real), \n",
    "# obtenemos el tamaño y dirección adecuados para ajustar los pesos.\n",
    "def sigmoid_derivative(output):\n",
    "    return output * (1 - output)\n",
    "\n",
    "# --- Inicialización ---\n",
    "peso = random.uniform(-1, 1)\n",
    "sesgo = random.uniform(-1, 1)\n",
    "tasa_aprendizaje = 0.1\n",
    "\n",
    "# --- Datos de entrenamiento ---\n",
    "entradas = [i / 10 for i in range(0, 21)]  # 0.0 hasta 2.0\n",
    "etiquetas = [1 if x > 1.0 else 0 for x in entradas]\n",
    "\n",
    "# --- Entrenamiento ---\n",
    "for epoca in range(1000):\n",
    "    perdida_total = 0\n",
    "    for x, y_esperado in zip(entradas, etiquetas):\n",
    "        \n",
    "        # Forward\n",
    "        z = x * peso + sesgo # Calcula la combinación lineal de la entrada. suma ponderada\n",
    "        y_predicho = sigmoid(z) # Aplica la función sigmoide para que la salida esté entre 0 y 1 (como probabilidad).\n",
    "\n",
    "        # Error\n",
    "        error = y_esperado - y_predicho # si esperabas 1 y predijo 0.8, el error es 0.2.\n",
    "        # Calcula el error cuadrático medio\n",
    "        perdida_total += error**2  \n",
    "\n",
    "        # Backpropagation - Esto calcula cuánto hay que ajustar los parámetros (peso y sesgo):\n",
    "        # Mide el impacto del error en el nodo, ajustado según la curva sigmoide.\n",
    "        # La derivada de la función sigmoide mide cuánto cambia la salida si cambiamos la entrada z un poquito.\n",
    "        # - Si la pendiente es grande (sigmoide no saturada), el delta será mayor, y los pesos se actualizarán más.\n",
    "        # - Si la pendiente es pequeña (sigmoide saturada), el delta será pequeño, y los pesos casi no cambiarán \n",
    "        #   (la neurona ya está “segura” de su respuesta).\n",
    "        delta = error * sigmoid_derivative(y_predicho)\n",
    "        grad_peso = delta * x # Cuánto debe cambiar el peso para reducir el error.\n",
    "        grad_sesgo = delta # Cuánto debe cambiar el sesgo.\n",
    "\n",
    "        # Actualización - Ajusta los parámetros usando descenso del gradiente:\n",
    "        # El peso y el sesgo se mueven en la dirección que disminuye el error.\n",
    "        peso += tasa_aprendizaje * grad_peso\n",
    "        sesgo += tasa_aprendizaje * grad_sesgo\n",
    "\n",
    "    if epoca % 100 == 0:\n",
    "        print(f\"Época {epoca} - Pérdida total: {perdida_total:.4f}\")\n",
    "\n",
    "# --- Pruebas ---\n",
    "print(\"\\nPruebas:\")\n",
    "for x in [0.5, 1.0, 1.1, 1.5, 2.0]:\n",
    "    salida = sigmoid(x * peso + sesgo)\n",
    "    pred = round(salida)\n",
    "    print(f\"x = {x:.1f} → {'Mayor' if pred == 1 else 'Menor o igual'} que 1\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "(6.478086950061453, -6.723905117048442)"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "peso, sesgo"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.11.0"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 2
}
